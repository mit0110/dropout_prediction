{"paragraphs":[{"text":"import java.io.File\nimport org.apache.spark.ml.feature.StringIndexer\nimport org.apache.spark.sql.SaveMode\nimport org.apache.spark.sql.functions.udf\nimport org.apache.spark.sql.types._","dateUpdated":"2017-12-15T10:29:19-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import java.io.File\nimport org.apache.spark.ml.feature.StringIndexer\nimport org.apache.spark.sql.SaveMode\nimport org.apache.spark.sql.functions.udf\nimport org.apache.spark.sql.types._\n"}]},"apps":[],"jobName":"paragraph_1513344559006_-1719369640","id":"20171213-114937_725604672","dateCreated":"2017-12-15T10:29:19-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3570"},{"text":"val BASE_DATA_DIRNAME = \"edm/data/kddcup2015\"\nval LOG_TRAIN_FILENAME = new java.io.File(BASE_DATA_DIRNAME, \"log_train.csv\").getPath\nval LOG_TEST_FILENAME = new java.io.File(BASE_DATA_DIRNAME, \"log_test.csv\").getPath\nval ENROLLEMENT_TRAIN_FILENAME = new java.io.File(BASE_DATA_DIRNAME, \"enrollment_train.csv\").getPath\nval ENROLLEMENT_TEST_FILENAME = new java.io.File(BASE_DATA_DIRNAME, \"enrollment_test.csv\").getPath\nval OBJECTS_FILENAME = new java.io.File(BASE_DATA_DIRNAME, \"object.csv\").getPath","dateUpdated":"2017-12-15T10:29:19-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"BASE_DATA_DIRNAME: String = edm/data/kddcup2015\nLOG_TRAIN_FILENAME: String = edm/data/kddcup2015/log_train.csv\nLOG_TEST_FILENAME: String = edm/data/kddcup2015/log_test.csv\nENROLLEMENT_TRAIN_FILENAME: String = edm/data/kddcup2015/enrollment_train.csv\nENROLLEMENT_TEST_FILENAME: String = edm/data/kddcup2015/enrollment_test.csv\nOBJECTS_FILENAME: String = edm/data/kddcup2015/object.csv\n"}]},"apps":[],"jobName":"paragraph_1513344559006_-1719369640","id":"20171213-114951_470299406","dateCreated":"2017-12-15T10:29:19-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3571"},{"text":"%md\n\n# Cleaning the data\n\nFirst, let's replace all the annoying hash strings on the dataset by numbers. For that, we use the `org.apache.spark.ml.feature.StringIndexer`. \n\nWe start by converting the course ids into numbers:","dateUpdated":"2017-12-15T10:29:19-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Cleaning the data</h1>\n<p>First, let&rsquo;s replace all the annoying hash strings on the dataset by numbers. For that, we use the <code>org.apache.spark.ml.feature.StringIndexer</code>. </p>\n<p>We start by converting the course ids into numbers:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1513344559006_-1719369640","id":"20171213-115033_366875059","dateCreated":"2017-12-15T10:29:19-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3572"},{"text":"val rawTrainErollmentDF = sqlContext.read.format(\"csv\")\n  .option(\"header\", \"true\")\n  .option(\"inferSchema\", \"true\")\n  .load(ENROLLEMENT_TRAIN_FILENAME)\n  \nval rawTestErollmentDF = sqlContext.read.format(\"csv\")\n  .option(\"header\", \"true\")\n  .option(\"inferSchema\", \"true\")\n  .load(ENROLLEMENT_TEST_FILENAME)","dateUpdated":"2017-12-15T10:29:19-0300","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"rawTrainErollmentDF: org.apache.spark.sql.DataFrame = [enrollment_id: int, username: string ... 1 more field]\nrawTestErollmentDF: org.apache.spark.sql.DataFrame = [enrollment_id: int, username: string ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1513344559007_-1719754389","id":"20171213-115127_768831285","dateCreated":"2017-12-15T10:29:19-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3573"},{"text":"rawTestErollmentDF.count()\nrawTrainErollmentDF.count()","dateUpdated":"2017-12-15T10:29:19-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res30: Long = 80362\nres31: Long = 120542\n"}]},"apps":[],"jobName":"paragraph_1513344559007_-1719754389","id":"20171213-115205_558795261","dateCreated":"2017-12-15T10:29:19-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3574"},{"text":"val courseIndexer = new StringIndexer()\n  .setInputCol(\"course_id\")\n  .setOutputCol(\"course_id_clean\")\n  .fit(rawTrainErollmentDF)\n\nval trainErollmentDF = courseIndexer.transform(rawTrainErollmentDF)\n    .select(\"enrollment_id\", \"username\", \"course_id_clean\").withColumnRenamed(\"course_id_clean\", \"course_id\")\n// We know for sure there is no course in the test dataset not present on the train dataset. It's enough\n// to train the indexer with rawTrainErollmentDF\nval testErollmentDF = courseIndexer.transform(rawTestErollmentDF)\n    .select(\"enrollment_id\", \"username\", \"course_id_clean\").withColumnRenamed(\"course_id_clean\", \"course_id\")","dateUpdated":"2017-12-15T10:29:19-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"courseIndexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_207eb729294b\ntrainErollmentDF: org.apache.spark.sql.DataFrame = [enrollment_id: int, username: string ... 1 more field]\ntestErollmentDF: org.apache.spark.sql.DataFrame = [enrollment_id: int, username: string ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1513344559008_-1733990098","id":"20171213-115352_1847019675","dateCreated":"2017-12-15T10:29:19-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3575"},{"text":"%md\n\nNow we fit a new StringIndexer for the module ids using the log files. There are three files involving module ids, and we need to use the modules in all of them to train the Indexer. ","dateUpdated":"2017-12-15T10:29:19-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Now we fit a new StringIndexer for the module ids using the log files. There are three files involving module ids, and we need to use the modules in all of them to train the Indexer.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1513344559008_-1733990098","id":"20171213-182711_1045382887","dateCreated":"2017-12-15T10:29:19-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3576"},{"text":"val rawTrainLogsDF = sqlContext.read.format(\"csv\")\n  .option(\"header\", \"true\")\n  .option(\"inferSchema\", \"true\")\n  .load(LOG_TRAIN_FILENAME)\n\nval rawTestLogsDF = sqlContext.read.format(\"csv\")\n  .option(\"header\", \"true\")\n  .option(\"inferSchema\", \"true\")\n  .load(LOG_TEST_FILENAME)\n\nval rawObjectsDF = sqlContext.read.format(\"csv\")\n  .option(\"header\", \"true\")\n  .option(\"inferSchema\", \"true\")\n  .load(OBJECTS_FILENAME)","dateUpdated":"2017-12-15T10:29:19-0300","config":{"colWidth":6,"results":{},"enabled":true,"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"rawTrainLogsDF: org.apache.spark.sql.DataFrame = [enrollment_id: int, time: timestamp ... 3 more fields]\nrawTestLogsDF: org.apache.spark.sql.DataFrame = [enrollment_id: int, time: timestamp ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1513344559008_-1733990098","id":"20171213-182932_1005922734","dateCreated":"2017-12-15T10:29:19-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3577"},{"text":"val allObjects = rawTrainLogsDF.selectExpr(\"object as object_id\").distinct.unionAll(\n    rawObjectsDF.selectExpr(\"module_id as object_id\").distinct.unionAll(\n        rawTrainLogsDF.selectExpr(\"object as object_id\").distinct))","dateUpdated":"2017-12-15T10:29:19-0300","config":{"colWidth":6,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"warning: there were two deprecation warnings; re-run with -deprecation for details\nallObjects: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [object_id: string]\n"}]},"apps":[],"jobName":"paragraph_1513344559009_-1734374847","id":"20171213-183405_1492161677","dateCreated":"2017-12-15T10:29:19-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3578"},{"text":"val objectIndexer = new StringIndexer()\n  .setInputCol(\"object_id\")\n  .setOutputCol(\"object_id_clean\")\n  .setHandleInvalid(\"skip\")\n  .fit(allObjects)","dateUpdated":"2017-12-15T10:29:19-0300","config":{"colWidth":6,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"objectIndexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_dbdcecb5305d\n"}]},"apps":[],"jobName":"paragraph_1513344559009_-1734374847","id":"20171213-183325_167383686","dateCreated":"2017-12-15T10:29:19-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3579"},{"text":"%md Now we apply the indexer to the rawLogs datasets","dateUpdated":"2017-12-15T10:29:19-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Now we apply the indexer to the rawLogs datasets</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1513344559009_-1734374847","id":"20171213-184853_703001309","dateCreated":"2017-12-15T10:29:19-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3580"},{"text":"val trainLogsDF = objectIndexer\n    .transform(rawTrainLogsDF.withColumnRenamed(\"object\", \"object_id\"))\n    .drop(\"object_id\")\n    .withColumnRenamed(\"object_id_clean\", \"object\")\ntrainLogsDF.cache()\ntrainLogsDF.count()","dateUpdated":"2017-12-15T10:29:19-0300","config":{"colWidth":6,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"trainLogsDF: org.apache.spark.sql.DataFrame = [enrollment_id: int, time: timestamp ... 3 more fields]\nres47: trainLogsDF.type = [enrollment_id: int, time: timestamp ... 3 more fields]\nres48: Long = 8157277\n"}]},"apps":[],"jobName":"paragraph_1513344559010_-1733220600","id":"20171213-183057_1994020541","dateCreated":"2017-12-15T10:29:19-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3581"},{"text":"val testLogsDF = objectIndexer\n    .transform(rawTestLogsDF.withColumnRenamed(\"object\", \"object_id\"))\n    .drop(\"object_id\")\n    .withColumnRenamed(\"object_id_clean\", \"object\")\ntestLogsDF.cache()\ntestLogsDF.count()","dateUpdated":"2017-12-15T10:29:19-0300","config":{"colWidth":6,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"testLogsDF: org.apache.spark.sql.DataFrame = [enrollment_id: int, time: timestamp ... 3 more fields]\nres49: testLogsDF.type = [enrollment_id: int, time: timestamp ... 3 more fields]\nres50: Long = 5387759\n"}]},"apps":[],"jobName":"paragraph_1513344559010_-1733220600","id":"20171213-185038_2012885671","dateCreated":"2017-12-15T10:29:19-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3582"},{"text":"%md \n\nThe last step is to apply both indexers to the object descriptions.\n\nAlso have into account that the objects dataframe has a column with a list of children objects for the corresponding module. As we also need to apply the StringIndexer to this list, we will explode and implode this list into a single value per row.","dateUpdated":"2017-12-15T10:29:19-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>The last step is to apply both indexers to the object descriptions.</p>\n<p>Also have into account that the objects dataframe has a column with a list of children objects for the corresponding module. As we also need to apply the StringIndexer to this list, we will explode and implode this list into a single value per row.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1513344559010_-1733220600","id":"20171213-185040_757624510","dateCreated":"2017-12-15T10:29:19-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3583"},{"text":"val moduleCleanDF = objectIndexer.transform(courseIndexer.transform(rawObjectsDF.na.fill(\"\")).withColumnRenamed(\"module_id\", \"object_id\"))\n    .withColumnRenamed(\"object_id_clean\", \"module_id_clean\")\n    .withColumnRenamed(\"object_id\", \"module_id\")\n    .dropDuplicates()\nmoduleCleanDF.count()\n\n// Convert the list of child nodes into a new row for each one, in order to apply the Object Indexer\nval rawObjectsDF2 = moduleCleanDF.withColumn(\"children_list\", split($\"children\", \" \"))\n  .withColumn(\"child\", explode($\"children_list\"))\n\nval objectsDF = objectIndexer\n  .transform(rawObjectsDF2.withColumnRenamed(\"child\", \"object_id\"))\n  .groupBy(\"module_id_clean\").agg(collect_list($\"object_id_clean\").as(\"children_clean\"))\n  .join(moduleCleanDF, Seq(\"module_id_clean\"), \"right\")\n  .selectExpr(\"course_id_clean as course_id\", \"module_id_clean as module_id\", \"category\", \"children_clean as children\", \"start\")\n\nobjectsDF.cache()","dateUpdated":"2017-12-15T10:29:19-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"moduleCleanDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [course_id: string, module_id: string ... 5 more fields]\nres52: Long = 26753\nrawObjectsDF2: org.apache.spark.sql.DataFrame = [course_id: string, module_id: string ... 7 more fields]\nobjectsDF: org.apache.spark.sql.DataFrame = [course_id: double, module_id: double ... 3 more fields]\nres57: objectsDF.type = [course_id: double, module_id: double ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1513344559011_-1733605349","id":"20171213-185359_1017533847","dateCreated":"2017-12-15T10:29:19-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3584"},{"text":"%md We save now all the processed datasets for later use.\n\nTo speed up future calculations, we will write the logs dataset divided by courses.","dateUpdated":"2017-12-15T10:29:19-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>We save now all the processed datasets for later use.</p>\n<p>To speed up future calculations, we will write the logs dataset divided by courses.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1513344559012_-1735529094","id":"20171213-185714_576561398","dateCreated":"2017-12-15T10:29:19-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3585"},{"text":"trainErollmentDF\n   .coalesce(1)  // place all data in a single partition \n   .write.format(\"com.databricks.spark.csv\")\n   .mode(SaveMode.Overwrite)\n   .option(\"header\", \"true\")\n   .save(new java.io.File(BASE_DATA_DIRNAME, \"clean/enrollment_train.csv\").getPath)\n  \ntestErollmentDF\n   .coalesce(1)  // place all data in a single partition \n   .write.format(\"com.databricks.spark.csv\")\n   .mode(SaveMode.Overwrite)\n   .option(\"header\", \"true\")\n   .save(new java.io.File(BASE_DATA_DIRNAME, \"clean/enrollment_test.csv\").getPath)","dateUpdated":"2017-12-15T10:29:19-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1513344559012_-1735529094","id":"20171213-115323_665236624","dateCreated":"2017-12-15T10:29:19-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3586"},{"text":"trainLogsDF\n   .write.format(\"com.databricks.spark.csv\")\n   .mode(SaveMode.Overwrite)\n   .option(\"header\", \"true\")\n   .save(new java.io.File(BASE_DATA_DIRNAME, \"clean/log_train.csv\").getPath)\n   \ntestLogsDF\n   .write.format(\"com.databricks.spark.csv\")\n   .mode(SaveMode.Overwrite)\n   .option(\"header\", \"true\")\n   .save(new java.io.File(BASE_DATA_DIRNAME, \"clean/log_test.csv\").getPath)","dateUpdated":"2017-12-15T10:29:19-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1513344559013_-1735913843","id":"20171213-190207_1175271835","dateCreated":"2017-12-15T10:29:19-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3587"},{"text":"val childrenList = udf((xs:Seq[Double]) => xs match {\n    case null => \"\"\n    case _ => xs.mkString(\" \")\n})\n\n// TODO delete this cell\nobjectsDF\n    .na.fill(\"\")\n    .withColumn(\"str_children\", childrenList($\"children\"))\n    .drop(\"children\")\n    .coalesce(1)  // place all data in a single partition\n    .write.format(\"com.databricks.spark.csv\")\n    .mode(SaveMode.Overwrite)\n    .option(\"header\", \"true\")\n    .save(new java.io.File(BASE_DATA_DIRNAME, \"clean/objects.csv\").getPath)","dateUpdated":"2017-12-15T10:29:19-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"childrenList: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(ArrayType(DoubleType,false))))\n"}]},"apps":[],"jobName":"paragraph_1513344559013_-1735913843","id":"20171213-185545_1618240512","dateCreated":"2017-12-15T10:29:19-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3588"},{"text":"%md\n\n# Machine learning dataset\n\nTo perform machine learning on this dataset, we are going to construct a single sequence instance with all the interactions of the same enrollment. ","dateUpdated":"2017-12-15T10:29:19-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Machine learning dataset</h1>\n<p>To perform machine learning on this dataset, we are going to construct a single sequence instance with all the interactions of the same enrollment.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1513344559014_-1734759596","id":"20171213-190434_1806808751","dateCreated":"2017-12-15T10:29:19-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3589"},{"text":"import org.apache.spark.sql.functions.{concat, lit}","dateUpdated":"2017-12-15T10:29:19-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.functions.{concat, lit}\n"}]},"apps":[],"jobName":"paragraph_1513344559014_-1734759596","id":"20171214-105816_430291035","dateCreated":"2017-12-15T10:29:19-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3590"},{"text":"val trainEnrollmentDF = sqlContext.read.format(\"csv\")\n  .option(\"header\", \"true\")\n  .option(\"inferSchema\", \"true\")\n  .load(new java.io.File(BASE_DATA_DIRNAME, \"clean/enrollment_train.csv\").getPath)\n  \nval trainLogsDF = sqlContext.read.format(\"csv\")\n  .option(\"header\", \"true\")\n  .option(\"inferSchema\", \"true\")\n  .load(new java.io.File(BASE_DATA_DIRNAME, \"clean/log_train.csv\").getPath)\n","dateUpdated":"2017-12-15T10:29:19-0300","config":{"colWidth":6,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"trainEnrollmentDF: org.apache.spark.sql.DataFrame = [enrollment_id: int, username: string ... 1 more field]\ntrainLabelsDF: org.apache.spark.sql.DataFrame = [1: int, 0: int]\n"}]},"apps":[],"jobName":"paragraph_1513344559014_-1734759596","id":"20171213-185451_503437991","dateCreated":"2017-12-15T10:29:19-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3591"},{"text":"val customSchema = StructType(Array(\n    StructField(\"enrollment_id\", IntegerType, true),\n    StructField(\"label\", IntegerType, true)))\n\nval trainLabelsDF = sqlContext.read.format(\"csv\")\n  .option(\"header\", \"true\")\n  .schema(customSchema)\n  .load(new java.io.File(BASE_DATA_DIRNAME, \"truth_train.csv\").getPath)","dateUpdated":"2017-12-15T10:29:19-0300","config":{"colWidth":6,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"customSchema: org.apache.spark.sql.types.StructType = StructType(StructField(enrollment_id,IntegerType,true), StructField(label,IntegerType,true))\ntrainLabelsDF: org.apache.spark.sql.DataFrame = [enrollment_id: int, label: int]\n"}]},"apps":[],"jobName":"paragraph_1513344559015_-1735144345","id":"20171213-190431_1873591312","dateCreated":"2017-12-15T10:29:19-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3592"},{"text":"trainLogsDF.show(3)","dateUpdated":"2017-12-15T10:29:19-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------+--------------------+-------+--------+------+\n|enrollment_id|                time| source|   event|object|\n+-------------+--------------------+-------+--------+------+\n|       123765|2013-12-15 04:32:...|browser|  access|3971.0|\n|       123765|2013-12-15 04:32:...|browser|   video|3501.0|\n|       123765|2013-12-20 16:42:...| server|navigate|5772.0|\n+-------------+--------------------+-------+--------+------+\nonly showing top 3 rows\n\n"}]},"apps":[],"jobName":"paragraph_1513344559015_-1735144345","id":"20171214-105408_1512586388","dateCreated":"2017-12-15T10:29:19-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3593"},{"dateUpdated":"2017-12-15T10:29:19-0300","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1513344559015_-1735144345","id":"20171214-105358_1784942413","dateCreated":"2017-12-15T10:29:19-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3594"},{"text":"val logSequences = trainLogsDF\n    .select($\"enrollment_id\", concat($\"object\".cast(\"string\"), lit(\"-\"), $\"event\").as(\"object_info\"))\n    .orderBy(\"time\")\n    .groupBy(\"enrollment_id\")\n    .agg(collect_list($\"object_info\").as(\"object_sequence\"))\n    .join(trainEnrollmentDF.select($\"enrollment_id\", $\"course_id\".cast(\"int\")), \"enrollment_id\")\n    .join(trainLabelsDF, \"enrollment_id\")\nlogSequences.chache()","dateUpdated":"2017-12-15T11:30:52-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"logSequences: org.apache.spark.sql.DataFrame = [enrollment_id: int, object_sequence: array<string> ... 2 more fields]\n<console>:52: error: value chache is not a member of org.apache.spark.sql.DataFrame\n       logSequences.chache()\n                    ^\n"}]},"apps":[],"jobName":"paragraph_1513344559016_-1737068089","id":"20171213-192140_1310974179","dateCreated":"2017-12-15T10:29:19-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3595"},{"text":"val sequenceList = udf((xs:Seq[String]) => xs match {\n    case null => \"\"\n    case _ => xs.mkString(\" \")\n})\n\n// TODO delete this cell\nlogSequences\n    .na.fill(\"\")\n    .withColumn(\"sequence\", sequenceList($\"object_sequence\"))\n    .drop(\"object_sequence\")\n    .write.format(\"com.databricks.spark.csv\")\n    .partitionBy(\"course_id\")\n    .mode(SaveMode.Overwrite)\n    .option(\"header\", \"true\")\n    .save(new java.io.File(BASE_DATA_DIRNAME, \"clean/train_sequences.csv\").getPath)","dateUpdated":"2017-12-15T10:29:19-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"sequenceList: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(ArrayType(StringType,true))))\n"}]},"apps":[],"jobName":"paragraph_1513344559017_-1737452838","id":"20171213-193808_819121819","dateCreated":"2017-12-15T10:29:19-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3596"},{"dateUpdated":"2017-12-15T10:29:19-0300","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1513344559017_-1737452838","id":"20171213-193854_1237241985","dateCreated":"2017-12-15T10:29:19-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3597"}],"name":"KDDCup 2015 Preprocessing","id":"2D32CSYTW","angularObjects":{"2CY1V66FH:shared_process":[],"2CUXM7W1H:shared_process":[],"2CUJVXDNZ:shared_process":[],"2CVPZ4W2G:shared_process":[],"2CW3JC3CR:shared_process":[],"2CWT5WUCQ:shared_process":[],"2CW6D4VRM:shared_process":[],"2CUMDWYJ6:shared_process":[],"2CUUXEW9P:shared_process":[],"2CYB6F8XS:shared_process":[],"2CV7AV6ZV:shared_process":[],"2CX2JXRME:shared_process":[],"2CXN99ACT:shared_process":[],"2CWQNEUPE:shared_process":[],"2CUYD59CN:shared_process":[],"2CVG5D8B6:shared_process":[],"2CUD9P4MP:shared_process":[],"2CWJ8555A:shared_process":[],"2CVB94AAS:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}